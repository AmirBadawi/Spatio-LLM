{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # DeepSeek Inference on Transport Mode Prediction\n",
    "#\n",
    "# This script runs inference using the local DeepSeek-R1-Distill-Qwen-14B model to predict transport modes from trip summaries.\n",
    "# It uses 4-bit quantization and QLoRA fine-tuning as described in your report.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import libraries for local model inference\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 200 records\n",
      "\n",
      "Transport mode distribution:\n",
      "transport_mode\n",
      "bike      40\n",
      "bus       40\n",
      "car       40\n",
      "others    40\n",
      "walk      40\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First few records:\n",
      "  transport_mode                                            summary\n",
      "0           bike  Trip Summary:\\n- Start: 2011-11-08 20:48:08 at...\n",
      "1           bike  Trip Summary:\\n- Start: 2008-04-26 04:43:58 at...\n",
      "2           bike  Trip Summary:\\n- Start: 2008-07-09 12:34:11 at...\n",
      "3           bike  Trip Summary:\\n- Start: 2008-08-12 02:18:58 at...\n",
      "4           bike  Trip Summary:\\n- Start: 2008-07-06 17:20:24 at...\n",
      "Loading model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B ...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m\n\u001b[1;32m     20\u001b[0m quant_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     21\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     22\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,          \u001b[38;5;66;03m# You can choose the quantization type: \"nf4\" or \"fp4\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16  \u001b[38;5;66;03m# Set the compute data type (e.g., torch.float16)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Load the model with the quantization config:\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeepseek-ai/DeepSeek-R1-Distill-Qwen-14B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/transformers/modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/transformers/modeling_utils.py:3698\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3695\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3698\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3704\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3705\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3706\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:75\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m     )\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available():\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m     )\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "with open('balanced_trip_summaries.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"Dataset size: {len(df)} records\")\n",
    "print(\"\\nTransport mode distribution:\")\n",
    "print(df['transport_mode'].value_counts())\n",
    "print(\"\\nFirst few records:\")\n",
    "print(df.head())\n",
    "\n",
    "# ## Setup Local DeepSeek-R1-Distill-Qwen-14B Model\n",
    "\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "print(f\"Loading model {MODEL_NAME} ...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "# Create a quantization configuration object:\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",          # You can choose the quantization type: \"nf4\" or \"fp4\"\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Set the compute data type (e.g., torch.float16)\n",
    ")\n",
    "\n",
    "# Load the model with the quantization config:\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_transport_mode(summary, max_new_tokens=10):\n",
    "    \"\"\"\n",
    "    Predict the transport mode from a trip summary using the local DeepSeek-R1-Distill-Qwen-14B model.\n",
    "    \n",
    "    Args:\n",
    "        summary (str): The trip summary text.\n",
    "        max_new_tokens (int): Maximum tokens to generate.\n",
    "        \n",
    "    Returns:\n",
    "        str: Predicted transport mode.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a transportation mode analysis expert. Analyze the trip summary below and determine the most likely mode of transportation used.\n",
    "\n",
    "Pay close attention to these key indicators:\n",
    "- Average speed and speed variations\n",
    "- Acceleration patterns\n",
    "- Number of turns and turn rates\n",
    "- Duration and distance\n",
    "- Start and end locations\n",
    "\n",
    "The trip summary includes various metrics that can help identify the transportation mode:\n",
    "- Walking typically has slow speeds (3-6 km/h), low acceleration, and potentially high turn rates\n",
    "- Biking usually shows moderate speeds (10-20 km/h), moderate acceleration, and varied turn patterns\n",
    "- Bus travel shows moderate speeds (15-30 km/h), lower acceleration, and fewer turns per km\n",
    "- Cars typically have higher speeds (30-80 km/h), higher acceleration capabilities, and varied turn patterns\n",
    "- Subway/train travel often has high speeds, very consistent acceleration/deceleration patterns\n",
    "\n",
    "Based on your analysis, classify the transportation mode as one of: bike, bus, car, subway, train, or walk.\n",
    "Only respond with the single word for the most likely transportation mode.\n",
    "\n",
    "Trip Summary:\n",
    "{summary}\n",
    "\"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    outputs = model.generate(input_ids, max_new_tokens=max_new_tokens, temperature=0.0, do_sample=False)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predicted_mode = generated_text.strip().lower()\n",
    "    \n",
    "    # Clean up the prediction if needed\n",
    "    if predicted_mode in ['bike', 'bicycle', 'cycling']:\n",
    "        return 'bike'\n",
    "    elif predicted_mode in ['bus', 'coach']:\n",
    "        return 'bus'\n",
    "    elif predicted_mode in ['car', 'taxi', 'automobile', 'drive']:\n",
    "        return 'car'\n",
    "    elif predicted_mode in ['subway', 'metro', 'underground']:\n",
    "        return 'subway'\n",
    "    elif predicted_mode in ['train', 'rail']:\n",
    "        return 'train'\n",
    "    elif predicted_mode in ['walk', 'walking', 'on foot']:\n",
    "        return 'walk'\n",
    "    else:\n",
    "        return predicted_mode\n",
    "\n",
    "# ## Test with a Single Example\n",
    "\n",
    "test_summary = df.iloc[0]['summary']\n",
    "true_mode = df.iloc[0]['transport_mode']\n",
    "\n",
    "print(f\"\\nTest summary:\\n{test_summary}\\n\")\n",
    "print(f\"True transport mode: {true_mode}\")\n",
    "\n",
    "# Uncomment the next lines when you're ready to run inference\n",
    "# predicted_mode = predict_transport_mode(test_summary)\n",
    "# print(f\"Predicted transport mode: {predicted_mode}\")\n",
    "\n",
    "# ## Run Inference on the Full Dataset\n",
    "#\n",
    "# Note: This will run inference for each example.\n",
    "\n",
    "def run_inference(df, sample_size=None):\n",
    "    \"\"\"\n",
    "    Run inference on the dataset and return results.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): The dataset.\n",
    "        sample_size (int): Optional sample size to limit inference.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Original dataframe with predictions added.\n",
    "    \"\"\"\n",
    "    if sample_size and sample_size < len(df):\n",
    "        df_sample = df.groupby('transport_mode').apply(\n",
    "            lambda x: x.sample(min(len(x), sample_size // len(df['transport_mode'].unique())))\n",
    "        ).reset_index(drop=True)\n",
    "        print(f\"Using a stratified sample of {len(df_sample)} examples\")\n",
    "    else:\n",
    "        df_sample = df\n",
    "        print(f\"Using all {len(df_sample)} examples\")\n",
    "    \n",
    "    results_df = df_sample.copy()\n",
    "    results_df['predicted_mode'] = None\n",
    "    \n",
    "    for idx, row in tqdm(results_df.iterrows(), total=len(results_df), desc=\"Running inference with local DeepSeek model\"):\n",
    "        summary = row['summary']\n",
    "        predicted_mode = predict_transport_mode(summary)\n",
    "        results_df.at[idx, 'predicted_mode'] = predicted_mode\n",
    "        \n",
    "    return results_df\n",
    "\n",
    "# Set this to a small number to test, for example, 60 (10 examples per mode for 6 modes)\n",
    "SAMPLE_SIZE = 80\n",
    "\n",
    "# Run inference\n",
    "# When ready to run, uncomment the line below:\n",
    "# results_df = run_inference(df, sample_size=SAMPLE_SIZE)\n",
    "\n",
    "# ## Evaluate Results\n",
    "\n",
    "def evaluate_results(results_df):\n",
    "    \"\"\"\n",
    "    Evaluate the results of the inference.\n",
    "    \n",
    "    Args:\n",
    "        results_df (DataFrame): DataFrame with true and predicted modes.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "    \n",
    "    y_true = results_df['transport_mode']\n",
    "    y_pred = results_df['predicted_mode']\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    classes = sorted(results_df['transport_mode'].unique())\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    per_class_acc = {}\n",
    "    for cls in classes:\n",
    "        mask = y_true == cls\n",
    "        if mask.sum() > 0:\n",
    "            per_class_acc[cls] = (y_pred[mask] == cls).mean()\n",
    "        else:\n",
    "            per_class_acc[cls] = 0\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=list(per_class_acc.keys()), y=list(per_class_acc.values()))\n",
    "    plt.axhline(y=accuracy, color='r', linestyle='--', label=f'Overall Accuracy: {accuracy:.4f}')\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Transport Mode')\n",
    "    plt.title('Per-Class Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'per_class_accuracy': per_class_acc,\n",
    "        'classification_report': classification_report(y_true, y_pred, output_dict=True)\n",
    "    }\n",
    "\n",
    "# When you have results, uncomment to evaluate\n",
    "# metrics = evaluate_results(results_df)\n",
    "\n",
    "# ## Save Results\n",
    "\n",
    "def save_results(results_df, metrics, model_name):\n",
    "    \"\"\"\n",
    "    Save the results and metrics to files.\n",
    "    \n",
    "    Args:\n",
    "        results_df (DataFrame): DataFrame with predictions.\n",
    "        metrics (dict): Evaluation metrics.\n",
    "        model_name (str): Name of the model used.\n",
    "    \"\"\"\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    filename = f\"{model_name}_predictions_{timestamp}.csv\"\n",
    "    results_df.to_csv(filename, index=False)\n",
    "    print(f\"Saved predictions to {filename}\")\n",
    "    \n",
    "    metrics_filename = f\"{model_name}_metrics_{timestamp}.json\"\n",
    "    with open(metrics_filename, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"Saved metrics to {metrics_filename}\")\n",
    "\n",
    "# When you have results, uncomment to save\n",
    "# save_results(results_df, metrics, MODEL_NAME)\n",
    "\n",
    "# ## Running the script\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Local DeepSeek Inference Script for Transport Mode Prediction\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\nTo run inference with the local DeepSeek model:\")\n",
    "    print(\"1. Ensure the DeepSeek model and tokenizer are properly installed and accessible.\")\n",
    "    print(\"2. Uncomment the results_df = run_inference() line.\")\n",
    "    print(\"3. Uncomment the metrics = evaluate_results() line.\")\n",
    "    print(\"4. Uncomment the save_results() line if you want to save the results.\")\n",
    "    \n",
    "    print(\"\\nNOTE: This script performs local inference and does not incur API costs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
